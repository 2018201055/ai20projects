# Task11 (NewsRec) Project I

GroupID: 18
StudentID: 2018202155
Submitted on 10.30

本项目主要是学习各种推荐系统模型，并基于现有模型算法作出进一步的改进。

我们目前已经分析了12/27个算法实例，其中我着重分析了以下6个算法：

1. Term Frequency - Inverse Document Frequency (TF-IDF)
2. LightFM/Hybrid Matrix Factorization
3. GeoIMC
4. xLearn/Factorization Machine (FM) & Field-Aware FM (FFM)
5. GRU4Rec
6. Short-term and Long-term preference Integrated Recommender (SLi-Rec)

 

## 搭建环境

1. 本地环境：安装带有3.6以上版本Python的Anaconda，用git将Recommerders仓库克隆到本地。然后创建并激活conda环境。

  

## 算法思路

**1. Term Frequency - Inverse Document Frequency (TF-IDF)**

用TF-IDF基于内容作推荐。该算法用到的是COVID-19 Open Research Dataset中license为cc0的数据，是文本类型的数据，数据量为258。

首先是数据预处理，将数据集中的原始数据转换为dataframe格式，选取cc0中的数据，并提取各文件里的完整文本内容。

然后文本分词，去除停用词，创建一个对象调用TfidfRecommend，训练模型，对每一个类型的参考文本返回与它相似度较高的前k个文本。其中，相似度的计算是基于TF-IDF算法，计算词频和逆文档频率，进而计算两篇文章之间的余弦相似值。

 

**2. LightFM/Hybrid Matrix Factorization**

推荐系统中会遇到冷启动问题（cold-start issue）也就是没有足够的历史数据，大致有3个类型：

1. 新平台，比如一个新网上购物平台，只有商品信息，没有用户、购买记录。

2. 新条目，比如新商品，缺乏访问次数，会导致推荐不准确，且缺少推荐，造成负反馈，导致流行偏见问题(popularity bias)

3. 新用户，缺乏访问或购买记录

为了解决这个问题，提出了组合协同过滤和内容推荐这两种推荐方法的混合推荐系统，其中之一是hybrid matrix factorisation模型，它组合了矩阵分解和LightFM包。

 

LightFM model:

U : 用户集

I : 物品集

每个用户有一系列用户特征
$$
f_u\subset F_U
$$
每个物品有一系列物品特征
$$
f_i\subset F_I
$$
该模型返回二值，评级会归一化为两组。对于显式评级，将用户物品对
$$
(u,i)\in U \times I
$$
分为正集合S+和负集合S-；对于隐性的反馈分为被观察的和不被观察的。

用户u和物品i的embedding为相应特征向量的和：
$$
q_u=\sum _{j\in f_u}e_j^U\\
p_i=\sum _{j\in f_i}e_j^I
$$
用户u和物品i的偏差为相应偏差向量的和：
$$
b_u=\sum _{j\in f_u}b_j^U\\
b_i=\sum _{j\in f_i}b_j^I
$$
 每个用户和物品表示为它特征向量的加权线性和。

预测用户u和物品i：
$$
r_{ui}=\sigma (q_u\cdot p_i+b_u+b_i)
$$
sigoid函数是进行归一化，以返回二值。

用随机梯度下降表示可能性，极大似然进行训模型拟合。可能性的表达式为：
$$
L=\prod _{(u,i)\in S^+}r_{ui}\times \prod_{(u,i)\in S^-}1-r_{ui}
$$



**3.  GeoIMC(Geometry Aware Inductive Matrix Completion)**

矩阵补全问题：有一个巨大的矩阵，人们只能观测到其中到部分元素，解决如何补全整个矩阵的问题。在推荐系统中，比如电影评分网站上，有很多用户（矩阵的行）给很多电影（矩阵的列）进行打分，但每个用户只会对很少一部分电影评分，那么矩阵中只有一部分值可被观测到，此时我们想预测用户对没有评分的电影的打分，也就是解决一个矩阵补全问题。

使用数据集MovieLens-100K，设
$$
X\in R^{m\times d_1} , Z\in R^{n\times d_2}
$$
分别为用户和电影的特征，
$$
U\in R^{m\times n}
$$
是部分观测到的评分矩阵。

GeoIMC将该矩阵建模为如下形式：
$$
M=XUBV^TZ^T
$$
其中
$$
U\in R^{d_1\times k},V\in R^{d_2\times k},B\in R^{k\times k}
$$
分别为正交矩阵、正交矩阵、对称正定矩阵。最优化问题通过Pymanopt解决。

 

 

**4. xLearn/Factorization Machine (FM) & Field-Aware FM (FFM)**

因子分解机，可处理高度稀疏的数据集，它不仅捕捉到输入的特征，而且**关注了特征与特征之间的相互关系** ，所以很强大。和其他传统算法比如SVM相比呈现出较好的泛化能力和表现。

最新的研究用深度学习方法延伸扩展了基本的FM算法，在几个应用实例中达到了显著的提升。

用户、物品、特征向量可表示为独热表达，此时传统的算法比如线性回归、SVM可能有以下问题：

1. 特征向量高度稀疏，因此，很难高效地优化参数拟合模型。

2. 特征的叉乘也会很稀疏，这样如果用它来刻画特征间的高阶交互，会降低模型的表现。

FM算法通过分解隐向量解决上述问题，它的大致思想是：
$$
y(x)=\omega_0+\sum _{i=1}^n \omega_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j
$$
其中x是输入的特征向量，y是待预测值，w_i是模型参数中的一阶元素，<vi,vj>是二阶相互关系，它是两个隐向量的点积，定义为：
$$
<v_i,v_j> =\sum_{f=1}^kv_{i,f}\cdot v_{j,f}
$$
计算复杂度为O(kn)，这里高阶交互元素中使用分解的向量与使用固定参数相比，可以提高模型的泛化能力和表现。

 

FFM是FM的一个延伸。直觉上FM中特征共用隐向量来表示不同类别的信息可能并不能很好地泛化相关度，FFM解决了这一问题，它在不同组的特征中使用了不同的分解隐向量，“组”在FFM中表述为'field'。FFM中二阶交互定义为：
$$
\theta_{FFM}(wx)=\sum_{j_1=1}^n\sum_{j_2=j_1+1}^n<v_{j_1,f_2},v_{j_2,f_1}>x_{j_1}x_{j_2}
$$
时间复杂度提高到了O(k*n^2)，但是FFM中的隐向量只需要在field内计算，所以FFM的k值通常会比FM中的小很多。

在本实验中，使用xlearn进行实现，它是用C++实现的，带Python接口，在能较高效地计算且不损失模型的有效性。



**序列推荐(sequential recommendation)**

以下五个模型都采用了序列推荐(sequential recommendation)，通过对用户（user）行为序列，比如购买商品（item）的序列来建模，学到user 兴趣的变化，从而能够对用户在短期内的行为或是下一次的行为进行预测。

- Attentive Asynchronous Singular Value Decomposition (A2SVD)

- Convolutional Sequence Embedding Recommendation (Caser)

- GRU4Rec

- Next Item Recommendation (NextItNet)

- Short-term and Long-term preference Integrated Recommender (SLi-Rec)

 

**5. GRU4Rec**

使用循环神经网络(recurrent neural network)捕捉用户的长期和短期偏好，在该模型中，使用完整的session行为序列信息，以session为粒度并行mini-batch，极大地加快了RNN-based模型训练。



**6. Short-term and Long-term preference Integrated Recommender (SLi-Rec)**

本实验中用到的是基于深度学习的SLi_Rec模型，它捕捉了长期的和短期的用户偏好以更精准地进行推荐，它有几个关键性质：

1. 长期兴趣的建模采用了注意力机制的不对称SVD；

2. 通过修改LSTM中的门控机制，同时考虑到了时间和语义的不规则性

3. 使用注意力机制动态地将长期偏好和短期偏好融合在一起。

   

## 后期规划

算法的代码运行中还有一些问题，之后我们将继续把给出的算法逐一理解原理并实现，再提出改进方法。

---

# Task11 (NewsRec) Project II
Submitted on 11.20

这个阶段我们分析了剩余的几个算法，其中我着重分析了以下7个算法：

1. Deep Knowledge-Aware Network (DKN)

2. Extreme Deep Factorization Machine (xDeepFM)
3. FastAI Embedding Dot Bias (FAST)
4. LightGCN
5. Attentive Asynchronous Singular Value Decomposition (A2SVD)
6. Convolutional Sequence Embedding Recommendation (Caser)
7. Neural Recommendation with Multi-Head Self-Attention (NRMS)



## 算法思路

**1. Deep Knowledge-Aware Network (DKN)**

**整体思路**: 用TransX知识表示学习，用一个CNN框架，对于一个新闻文章，将实体嵌入和单词嵌入结合起来产生一个最终的嵌入向量，作出点击率CTR(click-through-rate)的预测。用到了注意力机制来动态计算用户的历史数据。
**具体说明**: 考虑到新闻的两个特点，首先，新闻语言高度浓缩，且包含很多知识实体和常识，之前的模型大部分仅仅从语义层面(semantic level)进行表示学习，较少考虑新闻包含的外部知识，没有充分挖掘新闻文本在知识层面(knowledge level)的联系。此外，新闻有很强的时效性，一个好的新闻推荐算法应该能随用户的兴趣的改变做出相应的变化。DKN模型主将知识图谱实体嵌入表示与神经网络结合起来，首先使用一种融合了知识的卷积神经网络KCNN(knowledge-aware convolutional neural network)，将新闻的语义表示与知识表示融合起来形成新的embedding表示，再建立从用户的新闻点击历史到候选新闻的attention机制，选出得分较高的新闻推荐给用户。
知识图谱网络嵌入：
知识图谱由大量节点及节点之间的边组成，节点表示实体，边表示节点之间的关系，可以看作是许多三元组(head,relation,tail)构成的集合。知识图谱的网络嵌入是要用一个低维稠密的向量来表示节点，保证该向量包含了节点间的相似性关系以及网络的结构信息。目前已有的translation-based的嵌入表示方法有：
(1)TransE：设h,r,t 分别是head, relation, tail对应的向量，优化目标是使
$$
h+r\approx t
$$
评分函数是
$$
f_r(h,t)=||h+r-t||_2^2
$$
数值越小说明对应向量之间的关系越符合网络中实体节点间的关系。

(2)TransH:评分函数为
$$
f_r(h,t)=||h_\perp+r-t_\perp||_2^2\\其中h_\perp=h-w_r^Thw_r,t_\perp=t-w_r^Ttw_r,||w_r||_2=1
$$
(3)TransR: 使用一个映射矩阵，把不同的实体向量映射到相同的向量空间中,
$$
f_r(h,t)=||h_r+r-t_r||_2^2\\其中h_r=hM_r,t_r=tM_r
$$
(4)TransD: 新的映射方式，
$$
f_r(h,t)=||h_\perp+r-t_\perp||_2^2\\其中h_\perp=(r_ph_p^T+I)h,t_\perp=(r_pt_p^T+I)t
$$
h_p, r_p, t_p是另一组实体及关系的向量表示，I是单位矩阵。

网络嵌入表示方法的损失函数均为：
$$
L=\sum_{(h,r,t)\in\Delta}\sum_{(h^{'},r,t^{'})\in\Delta^{'}}max(0,f_r(h,t)+\gamma-f_r(h^{'},t^{'}))
$$

**2. Extreme Deep Factorization Machine (xDeepFM)**

基于Field的vector-wise思想引入Cross，并且保留了Cross的优势。

CIN是在vector-wise level，DNN是在bit-wise level


整体思路：

(1)对原始特征的Field形式包装，把特征one-hot形式包装进同一个field来克服稀疏性；
(2)在embeding层对样本做embeding转换，按照 deepFM相似的形式来获取每个样本长度为 field_size的 embedding表示，这样embedding后的样本矩阵为(field_size, embedding_size)；
(3)简单的一阶计算部分，这部分没有用到特征embedding的结果，而是用w\*x那种类似LR的一阶计算;
(4)CIN模型(Compressed Interaction Network)进行可控的自动学习显式的高阶特征交互，运用deep & cross思想，CIN层的输入来自Embedding层，设有m 个field，每个field的embedding vector维度为D, 则输入可表示为矩阵
$$
X^0\in R^{m*D}
$$
(5)最后是 DNN部分的结构，对经过embedding转换后的样本特征进行隐式的高阶特征交互。



其中比较重要的部分是CIN层，每层的计算方法为：

令
$$
X^k\in R^{H_k*D}
$$
表示第k层的vector个数，其中H_k表示第k层的vector个数，vecor维度始终为D，保持和输入层一致。第k层每个vector的计算方式为：
$$
X_{h,*}^k=\sum_{i=1}^{H_{k-1}}\sum_{j=1}^m W_{ij}^{k,h}(X_{i,*}^{k-1}\omicron X_{j,*}^0)\in R^{1*D}, where\ 1\le h\le H_k
$$
其中W^{k,h}表示第k层的第h个vector的权重矩阵，空心圈表示Hadamard乘积，即逐元素乘。这个公式取前一层X^{k-1}中的H_{k-1}个vector，与输入层X^0中的m个vector，进行两两Hadamard乘积运算，得到H\_{k-1}\*m个 vector，然后加权求和。第k层的不同vector区别在于，对这H\_{k-1}\*m个 vector 求和的权重矩阵不同。H_k即对应有多少个不同的权重矩阵W^k, 是一个可以调整的超参。

**3. FastAI Embedding Dot Bias (FAST)**

提供一个封装的框架，用到嵌入(embedding)、对用户和物品的偏置(bias)。

**4. LightGCN**

深度学习算法，是GCN预测隐式反馈的简化方法，只包含了GCN中最基本的邻域聚集部分，更适合推荐系统。

整体思路：先将用户和物品节点的领域聚合，再使用三层卷积层分别生成每层的嵌入，然后将节点的原始输入与生成的每层的嵌入做加权和，最后将用户和物品最终的生成节点表示做内积，生成预测的分数。

计算过程：

使用节点的邻域加权运算下一层的节点表示：
$$
e_u^{(k+1)}=\sum_{i\in N_u}\frac 1 {\sqrt{|N_u||N_i|}}e_i^{(k)}\\
e_i^{(k+1)}=\sum_{u\in N_i}\frac 1 {\sqrt{|N_i||N_u|}}e_u^{(k)}
$$
将每层生成的节点表示累加，生成最终的节点表示：
$$
e_u=\sum _{k=0}^K\alpha_ke_u^{(k)}\\
e_i=\sum _{k=0}^K\alpha_ke_i^{(k)}\\
其中设\alpha_k=\frac 1 {(K+1)}
$$
最后根据生成的节点表示做内积得到最终的预测分数：
$$
y_{ui}=e_u^Te_i
$$


**5. Attentive Asynchronous Singular Value Decomposition (A2SVD)**

动态地将用户的长期和短期偏好结合起来，将传统的RNN模型做改进来更好地用于个性化推荐系统。主要提出了两个方法：

1. 直觉上，在短时间间隔内的两个动作比长时间间隔内的两个动作关系更紧密，所以设置一个time-aware controller来动态刻画时间上的信息。

2. 用户行为背后的用户意图总是在动态变化，不相关的动作对于预测将来的行为没有用处，所以需要动态描述潜在意图，设置一个content-aware controller来动态刻画上下文信息。

并且进一步基于attention机制将用户的短期和长期偏好结合起来。


Short-Term Modeling：

对于时间的不规律性：在传统的LSTM中引入两个时间特征，时间间隔delta_t_k和时间跨度s_t_k，然后计算相应的时间门控T_delta和T_s。

对于语义的不规律性：采用注意力机制压制那些偏离目标方向的信息。

将用户用所有算得的隐状态的加权和来表示。

Long-Term Modeling：

将用户用与他们有过交互的物品来表示。

短期和长期偏好的结合方法：采用动态构建的方法，它们各自占的比重取决于特定的环境，比如什么时候（如果下一个动作紧接着上一个动作，那么短期偏好的特征更有用），或者什么内容（比如关于手机的偏好可能用长期偏好刻画更有效，手机配件可能用短期偏好刻画更有效）



**6. Convolutional Sequence Embedding Recommendation (Caser)**

基于卷积捕捉用户的一般偏好和序列模式。将每个用户有过交互的物品看作一个序列，用来预测用户将来可能有交互的前n个物品的排序，Caser模型的主要思想是将与当前物品在时间和空间上相近的商品形成一个“图像”，用卷积滤波器学习序列模式作为图像的局部特征。



具体方法：

用卷积神经网络(CNN)学习序列特征，用隐因子模型(LFM)学习用户特征，caser由三部分组成：e mbedding layer, convolutional layer, fully-connected layer，对于每个用户u，从用户序列S_u中提取每L个连续项作为输入，将他们的下T项作为目标，将长为L的序列映射为embedding，堆积在一起，而用户特征是用LFM生成。水平卷积层有n个水平滤波器，在该例中，有8个滤波器，每个h={1,2,3,4}各自对应两个滤波器，Fk从上到下滑动，与物品的横向维度发生相互作用， 再对卷积的结果做最大池化操作。垂直卷积网络中每个滤波器Fk与列交互从左到右滑动。在全连接层将两个卷积层的结果连接起来，进行嵌入得到z，z包含了之前项的所有序列特征，为了捕捉用户的一般偏好，嵌入用户P_u，将z和P_u连接，再将他们投射到有|I|个节点的输出层，
$$
y^{(u,t)}=W'\left[\begin{matrix}z\\P_u\end{matrix}\right]+b'
$$



**7. Neural Recommendation with Multi-Head Self-Attention (NRMS)**

使用multi-head self-attention方法，NRMS的核心是一个新闻编码器和一个用户编码器，在新闻编码器中，通过构建单词之间的互动来学习新闻的表示，在用户编码器中，通过捕捉用户浏览过的新闻之间的关系来学习用户的表示。并用另外的注意力机制通过挑选重要的单词和新闻来学习那些更能提供信息的新闻和用户的表示。



## 后期规划

目前已分析完给出的推荐系统的算法，但是在代码运行中还存在一些问题，下一阶段先把代码中的问题进行修复，然后分析不同方法的性能，比较各自的优劣势，提出改进算法。


----

# Task11 (NewsRec) Project III
Submitted on 12.18

本阶段我们提出了改进算法，然后分析了基于MIND数据集的新闻推荐比赛中获奖的7个改进算法的思路。

其中我着重分析了前三篇文章：
MIND News Recommendation Competition基于MIND数据集，根据四周的新闻数据预测第五周的用户点击。

**1.Huige Cheng, Sogou, China (chenghuige)**
模型方法：
将用户观察到的曝光物品(impressions，即推荐系统推荐的物品)进行shuffle可以得到更好的结果

- 对于历史点击新闻：将docid, cats,subcats做attention pooling，将entity,title,abstract,body做DIN attention pooling
- 对于候选新闻文档：将title, abstract, body做attention pooling
然后将计算好的特征做拼接、内积运算，最后放入MLP中训练
附：attention pooling:即在pooling层做attention，用CNN学习到特征，然后产生权重，进行attention，最后得到表达向量。
DIN attention pooling：对于不同的新闻赋予不同权重的attention pooling

评估的方法：
测试集中的文档比例远高于验证集中的，因此如果算法高度依赖于docid，会导致在验证集中得到好的结果而在测试集中效果不好。但是docid对验证集仍有作用所以仍采用docid作为输入的特征。而为了让验证集和测试集在效果上一致，随机地将验证集中的一部分docid改为在训练集中不出现，解决这一问题。
用auc方法评估，采用异步验证，即边训练边得到评估的结果。

**2.U Kang, Seoul National University and DeepTrade Inc., South Korea (dtsnu)**
模型方法：
整体上是基于multi-head self-attention(NRMS)和attentive multi-view learning(NAML)这两种算法，用BERT、Glove从title、category、subcategory、abstract中提取词嵌入，然后用两层多头自注意力提取题目和摘要的更复杂的嵌入，把这几个嵌入用带注意力的多视角结合起来，最终基于这个基本模型做集成。

- NRMS+NAML+Glove: 多头自注意力机制能反映序列中所有的单词长度的信息，所以它比CNN、RNN能表示文本中更复杂的样式；同时，新闻的摘要、分类等辅助信息能帮助提高推荐性能，所以将这两种算法进行结合。News Encoder由四部分组成：title encoder, abstract encoder, category encoder, sub-category encoder，其中题目和摘要编码器是分别从题目和摘要中获得词嵌入（用Glove初始化），然后用多头自注意力网络学习上下文的表达；category encoder和sub-category encoder也是先从类别和子类别中获得嵌入向量，然后用全联接神经网络学习隐藏表达。由这四个编码器编码得到的向量用一个带注意力机制的multi-view network连接起来。而对于用户编码器和预测点击率采用和NRMS相同的方法。

- NRMS+NAML+BERT: 整体上和第一个模型一样，用BERT得到更复杂的词嵌入，但是由于参数量大，训练所需时间长，所以对BERT的参数做了修改，在BERT顶部用非线性的激活函数堆叠三个MLP层，来高效地提取有用特征。

然后基于这两个基本模型做集成学习(ensemble)。
对189个集成后的模型在验证集上比较，挑选出表现最好的4个，在测试集上测试，模型呈现出的泛化能力较好，测试集上的表现比验证集的更好。


**3.Yichao Lu, Layer 6 AI, Canada (oahciy)**
模型方法：
- 基于bidirectional LSTM （同时使用正向的LSTM和反向的LSTM）做一些改进：用fastText代替Glove做词向量的初始化、对LSTM的权重正交初始化，对输入的文本扩大数据量，可以通过打乱、替换、使用同义词等方法，并用LazyAdam（仅更新当前batch中出现的稀疏变量索引的移动平均累加器，而不是更新所有索引的累加器）方法更新稀疏的词嵌入。
- 位置的嵌入：由于自注意力网络模型不能感知序列的位置信息，而用户浏览新闻的顺序对描述短期用户偏好很重要，所以提出一个方法让自注意力能感知位置，由此引入一个位置嵌入层，它把每个位置编码为一个向量表达。另外，进入位置嵌入层之前要将用户浏览历史翻转，以处理变长的用户历史数据，这样能使得所有用户的最后一个浏览或倒数第二个...新闻用的是同一个位置嵌入层。
- 添加多视角表示学习
- 自注意力层使用相同的参数，在实验数据集中能呈现较好的表现，因为使用相同参数可能会使模型规则化；另外将点积注意力进行缩放能增强模型。NRMS模型计算第i个用户和第j个新闻之间的相关性分数的公式为：

$$
a_{i,j}^k=\frac {exp(e_i^TQ_ke_j)} {\sum {_{m=1}^M}exp(e_i^TQ_ke_m)},\\
h_{i,k}=V_k(\sum _{j=1}^M {a_{i,j}^k e_j}
)
$$

其中$a_{i,j}^k$为用第k个注意力头学到的第i个用户和第j个新闻相关性分数之间的关系分数，$Q_k$和$V_k$是投影矩阵。
修改为该方法后的公式为：
$$
a_{i,j}^k=\frac {exp( \frac 1 {\sqrt{d_k}}(e_i^T Q_k^T K_k e_j)} {\sum {_{m=1}^M}exp(\frac 1 {\sqrt{d_k}} e_i^T Q_k^T K_k e_m)},\\
h_{i,k}=K_k(\sum _{j=1}^M {a_{i,j}^k e_j}
)
$$
其中$ \frac 1 {\sqrt{d_k}} $ 是一个缩放的因子。

- 集成(ensemble)
集成基本模型的方法有bagging和boosting，bagging均匀取样，每个样例的权重相等，能在一定程度上降低variance，boosting根据错误不断调整样例权重，错误率大的权重大，不断最小化损失函数，可以减小bias。由于NRMS是基于神经网络的模型，所以它的bias比较低variance比较高，所以选择bagging更好。使用scaled probabilities模型集成，即使用未归一化的softmax激活函数作为集成模型的预测值，因为scaled probability能更准确地反映单个模型地预测点击率的置信度。


**改进思路：**
基于以下三个算法：
LSTUR: 主要包括两个编码器：新闻编码器、用户编码器。新闻编码器主要用到的思路为word embedding，CNN以及attention机制；用户编码器根据用户的embedding学习长期兴趣，使用GRU从用户最近浏览的新闻中学习短期用户兴趣，然后结合用户的长短期兴趣。

NAML(multi-view)：包括新闻编码器和用户编码器，新闻编码器分为三个部分category，title，body。用户编码器将用户浏览的历史记录输入到新闻编码器中进行编码后利用attention对不同新闻加权处理。

NRMS: 多头自注意力机制通过挑选重要的单词和新闻来学习那些更能提供信息的新闻和用户的表示。

所以我们基于这三个算法设计改进思路，对于用户：刻画长期和短期兴趣；对于新闻的表达：采用multi-view，更准确地表示新闻；另外用注意力机制更多地关注重要的新闻历史数据。

算法代码结构：
- base_model.py 定义BaseModel类，做初始化，调用模型的训练、评估函数，调用新闻编码器、用户编码器。
- layers.py 定义自注意力层，初始化，计算输出参数，及建立和调用层的函数。
- model.py 模型的主要部分，定义Model类，初始化，定义标签，定义用户和新闻的特征，建立用户编码器和新闻编码器，调用前面各部分模型的总模型

## 总结
这次项目首先学习了目前比较流行的一些新闻推荐算法，然后提出了自己的改进思路，整体上完成了初始任务，但是在最后算法的运行上，只设计了算法并写了模型代码，在训练数据上出现了难以解决的问题，将这部分的工作代替为学习其他人如何基于已有算法做性能上的改进，分析了改进思路和实验效果，了解了有效的改进方法。

通过一个学期的项目收获很大，学习的内容基本上涵盖到常用到的深度学习模型，对新闻推荐系统的框架和各种算法思路有了较全面的了解，也对一些实验效果较好的算法思路做了细致的分析，建立了自己的思考和认识，之后继续对这方面进行更深刻的学习和理解。