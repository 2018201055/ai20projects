{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG提取图片特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "import string\n",
    "import os\n",
    "from os import listdir, mkdir\n",
    "from pickle import dump, load\n",
    "import zhon.hanzi\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, TimeDistributed, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "def extract_features(directory):\n",
    "    # 去除最后一层，因为目的不是分类，而是特征抽取\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    model.summary()\n",
    "    \n",
    "    # 每张图片抽取特征\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # 将像素转存为数组形式\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # VGG的函数，预处理并预测\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        #存储图片特征\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        # print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "features = extract_features('Data/Images')\n",
    "print('Extracted Features: %d' % len(features))\n",
    "\n",
    "if not os.path.exists('MiddleFiles'):\n",
    "    mkdir('MiddleFiles')\n",
    "dump(features, open('MiddleFiles/features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取文本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8091\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# 用字典存储标题-语句信息\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        # 以空格分隔\n",
    "        tokens = line.split()\n",
    "        # 第一个是图片标题，后面的是描述语句\n",
    "        image_id, image_desc = tokens[0], tokens[1]\n",
    "        if image_id[-1] != '0':\n",
    "            continue\n",
    "        # 去除.jpg的后缀\n",
    "        image_id = image_id.split('.')[0]\n",
    "        #以字典形式存储\n",
    "        mapping[image_id] = image_desc\n",
    "    return mapping\n",
    "\n",
    "#保存文件\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "doc = load_doc('Data/Captions/flickr8kzhc.caption.txt')\n",
    "\n",
    "# 加载描述语句\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d' % len(descriptions))\n",
    "\n",
    "# 保存文件\n",
    "if not os.path.exists('MiddleFiles'):\n",
    "    mkdir('MiddleFiles')\n",
    "save_descriptions(descriptions, 'MiddleFiles/descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.919 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions_train: 6000\n",
      "Descriptions_val: 1000\n",
      "Descriptions: 7000\n",
      "Photos_train: 6000\n",
      "Photos_val: 1000\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集的图片名称\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        # 可能有空行\n",
    "        if not line:\n",
    "            continue\n",
    "        dataset.append(line)\n",
    "    return set(dataset)\n",
    "\n",
    "# 根据图片名称加载描述语句\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    punctuations = string.punctuation + zhon.hanzi.punctuation\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1]\n",
    "        image_desc = jieba.cut(image_desc)\n",
    "        image_desc = [word for word in image_desc if word not in punctuations]\n",
    "        \n",
    "        # 描述语句首尾加开始/终止词，用于序列输入\n",
    "        image_desc.insert(0, 'startseq')\n",
    "        image_desc.append('endseq')\n",
    "        \n",
    "        # 跳过不在训练集中的图片\n",
    "        if image_id in dataset:\n",
    "            descriptions[image_id] = image_desc\n",
    "    return descriptions\n",
    "\n",
    "# 根据图片名称加载图片的特征向量\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k].reshape(1, 49, 512) for k in dataset}\n",
    "    return features\n",
    "\n",
    "\n",
    "#加载图片名称\n",
    "image_ids_train = load_set('Data/Partition/flickr8ktrain.txt')\n",
    "image_ids_val = load_set('Data/Partition/flickr8kval.txt')\n",
    "image_ids_test = load_set('Data/Partition/flickr8ktest.txt')\n",
    "\n",
    "image_ids = set.union(image_ids_train,image_ids_val)\n",
    "\n",
    "print('Dataset: %d' % len(image_ids_train))\n",
    "\n",
    "#通过图片名称得到图片的描述语句\n",
    "descriptions_train = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids_train)\n",
    "print('Descriptions_train: %d' % len(descriptions_train))\n",
    "\n",
    "descriptions_val = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids_val)\n",
    "print('Descriptions_val: %d' % len(descriptions_val))\n",
    "\n",
    "descriptions = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids)\n",
    "print('Descriptions: %d' % len(descriptions))\n",
    "\n",
    "\n",
    "#通过图片名称得到图片的特征向量\n",
    "features_train = load_photo_features('MiddleFiles/features.pkl', image_ids_train)\n",
    "print('Photos_train: %d' % len(features_train))\n",
    "\n",
    "#通过图片名称得到图片的特征向量\n",
    "features_val = load_photo_features('MiddleFiles/features.pkl', image_ids_val)\n",
    "print('Photos_val: %d' % len(features_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建单词映射与序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2260\n",
      "Description Length: 20\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 建立分词器tokenizer\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def one_hot_encode(data, MAXIMUM_CAPTION_LENGTH, n_classes):\n",
    "    result = np.zeros((len(data), MAXIMUM_CAPTION_LENGTH, n_classes))\n",
    "    for i, item in enumerate(data):\n",
    "        a = 0\n",
    "        for j, word in enumerate(item):\n",
    "            result[i, j, word] = 1.0\n",
    "            a = j\n",
    "        for k in range(a+1, MAXIMUM_CAPTION_LENGTH):\n",
    "            result[i, k, 0] = 1.0\n",
    "    return result\n",
    "\n",
    "def data_generator(batch_size, captions, get_image, tokenizer, max_length, vocab_size):\n",
    "    n_steps = int(np.ceil(len(captions) / batch_size))\n",
    "    while True:\n",
    "        for i in range(n_steps):\n",
    "            if i < n_steps - 1:\n",
    "                batch_indices = np.arange(i, i + batch_size)\n",
    "            else:\n",
    "                batch_indices = np.arange(64 * (n_steps - 1), len(captions))\n",
    "        \n",
    "            L = list(captions.keys())\n",
    "\n",
    "            batch_image_features = np.empty((len(batch_indices), 7*7, 512))\n",
    "            for i, j in enumerate(batch_indices):\n",
    "                batch_image_features[i] = get_image[L[j]].reshape((7*7, 512))\n",
    "            \n",
    "            batch_captions1 = [captions[L[item]][:-1] for item in batch_indices]\n",
    "            batch_captions2 = [captions[L[item]][1:] for item in batch_indices]\n",
    "\n",
    "            input_captions = tokenizer.texts_to_sequences(batch_captions1)\n",
    "            output_captions = tokenizer.texts_to_sequences(batch_captions2)\n",
    "\n",
    "            input_captions = pad_sequences(input_captions, maxlen= max_length, padding='post')\n",
    "            output_captions = one_hot_encode(output_captions, max_length, vocab_size)\n",
    "       \n",
    "            batch_image_features = np.array(batch_image_features, dtype=np.float32)\n",
    "\n",
    "            x_data = [input_captions, batch_image_features]\n",
    "            y_data = output_captions\n",
    "\n",
    "            yield (x_data, y_data)\n",
    "\n",
    "\n",
    "#输出所有语句不同的单词总数\n",
    "tokenizer = create_tokenizer(descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "#输出单个语句最大长度\n",
    "max_length = max([len(desc) for desc in descriptions.values()])\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 49, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 25088)        0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 128)          3211392     flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128)          0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 20, 256)      578560      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 20, 128)      0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 20, 128)      197120      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 20, 256)      0           repeat_vector_7[0][0]            \n",
      "                                                                 lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 20, 256)      65792       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 20, 2260)     580820      dense_20[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,633,684\n",
      "Trainable params: 4,633,684\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "94/94 [==============================] - 5s 52ms/step - loss: 1.6416 - accuracy: 0.1778 - val_loss: 1.6186 - val_accuracy: 0.2155\n",
      "Epoch 2/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 1.1401 - accuracy: 0.3117 - val_loss: 1.7539 - val_accuracy: 0.2226\n",
      "Epoch 3/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.9167 - accuracy: 0.4058 - val_loss: 1.8017 - val_accuracy: 0.2430\n",
      "Epoch 4/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.7127 - accuracy: 0.5177 - val_loss: 1.7671 - val_accuracy: 0.2251\n",
      "Epoch 5/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.5536 - accuracy: 0.6069 - val_loss: 1.7352 - val_accuracy: 0.2196\n",
      "Epoch 6/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.4501 - accuracy: 0.6724 - val_loss: 1.7111 - val_accuracy: 0.2114\n",
      "Epoch 7/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.3877 - accuracy: 0.7124 - val_loss: 1.7261 - val_accuracy: 0.2320\n",
      "Epoch 8/30\n",
      "94/94 [==============================] - 4s 43ms/step - loss: 0.3436 - accuracy: 0.7362 - val_loss: 1.7197 - val_accuracy: 0.2286\n",
      "Epoch 9/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.3177 - accuracy: 0.7495 - val_loss: 1.7372 - val_accuracy: 0.2464\n",
      "Epoch 10/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.3004 - accuracy: 0.7573 - val_loss: 1.7395 - val_accuracy: 0.2363\n",
      "Epoch 11/30\n",
      "94/94 [==============================] - 5s 50ms/step - loss: 0.2908 - accuracy: 0.7641 - val_loss: 1.7508 - val_accuracy: 0.2473\n",
      "Epoch 12/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.2820 - accuracy: 0.7690 - val_loss: 1.7724 - val_accuracy: 0.2471\n",
      "Epoch 13/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.2761 - accuracy: 0.7701 - val_loss: 1.7637 - val_accuracy: 0.2487\n",
      "Epoch 14/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.2724 - accuracy: 0.7712 - val_loss: 1.7860 - val_accuracy: 0.2506\n",
      "Epoch 15/30\n",
      "94/94 [==============================] - 4s 43ms/step - loss: 0.2688 - accuracy: 0.7732 - val_loss: 1.7772 - val_accuracy: 0.2509\n",
      "Epoch 16/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2686 - accuracy: 0.7704 - val_loss: 1.7776 - val_accuracy: 0.2405\n",
      "Epoch 17/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2673 - accuracy: 0.7705 - val_loss: 1.7774 - val_accuracy: 0.2474\n",
      "Epoch 18/30\n",
      "94/94 [==============================] - 4s 39ms/step - loss: 0.2657 - accuracy: 0.7702 - val_loss: 1.7857 - val_accuracy: 0.2385\n",
      "Epoch 19/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2656 - accuracy: 0.7691 - val_loss: 1.7741 - val_accuracy: 0.2490\n",
      "Epoch 20/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2658 - accuracy: 0.7676 - val_loss: 1.7855 - val_accuracy: 0.2426\n",
      "Epoch 21/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2660 - accuracy: 0.7682 - val_loss: 1.7944 - val_accuracy: 0.2453\n",
      "Epoch 22/30\n",
      "94/94 [==============================] - 4s 39ms/step - loss: 0.2640 - accuracy: 0.7683 - val_loss: 1.8014 - val_accuracy: 0.2471\n",
      "Epoch 23/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.2647 - accuracy: 0.7680 - val_loss: 1.7944 - val_accuracy: 0.2520\n",
      "Epoch 24/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.2637 - accuracy: 0.7684 - val_loss: 1.7961 - val_accuracy: 0.2417\n",
      "Epoch 25/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2637 - accuracy: 0.7673 - val_loss: 1.7951 - val_accuracy: 0.2520\n",
      "Epoch 26/30\n",
      "94/94 [==============================] - 4s 42ms/step - loss: 0.2619 - accuracy: 0.7688 - val_loss: 1.8115 - val_accuracy: 0.2416\n",
      "Epoch 27/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.2632 - accuracy: 0.7679 - val_loss: 1.8153 - val_accuracy: 0.2467\n",
      "Epoch 28/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.2634 - accuracy: 0.7675 - val_loss: 1.8302 - val_accuracy: 0.2497\n",
      "Epoch 29/30\n",
      "94/94 [==============================] - 4s 40ms/step - loss: 0.2628 - accuracy: 0.7678 - val_loss: 1.8328 - val_accuracy: 0.2490\n",
      "Epoch 30/30\n",
      "94/94 [==============================] - 4s 41ms/step - loss: 0.2637 - accuracy: 0.7650 - val_loss: 1.8322 - val_accuracy: 0.2470\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Lambda,add,RepeatVector\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # 以下是文本特征提取层\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se3 = LSTM(128,return_sequences=True)(se1)\n",
    "    \n",
    "    # 以下是图片特征提取层(其实输入的已经是提取好的特征了)\n",
    "    inputs1 = Input(shape=(49, 512))\n",
    "    fe1 = Flatten()(inputs1)\n",
    "    fe2 = Dense(128, activation='relu')(fe1)\n",
    "    fe2 = Dropout(0.5)(fe2)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "    \n",
    "    # 以下是结果输出层\n",
    "    decoder1 = concatenate([fe3, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # 输入为图片+文本，输出为单个单词\n",
    "    model = Model(inputs=[inputs2, inputs1], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "    \n",
    "    # 模型描述与绘制\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# 保存模型\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "steps = len(descriptions)\n",
    "batch_size = 64\n",
    "\n",
    "# create the data generator\n",
    "generator = data_generator(batch_size, descriptions_train, features_train, tokenizer, max_length, vocab_size)\n",
    "validation_generator = data_generator(batch_size, descriptions_val, features_val, tokenizer, max_length, vocab_size)\n",
    "\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(generator, epochs=30, validation_data=validation_generator,\n",
    "                              validation_steps=int(np.ceil(len(descriptions_val) / batch_size)),\n",
    "                              steps_per_epoch=int(np.ceil(len(descriptions_train) / batch_size)), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 数字映射到单词\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# 把模型预测转换成语句\n",
    "def generate_desc(model, tokenizer, photo, max_length): ## 需要修改!!!!!\n",
    "    in_text = 'startseq'\n",
    "    # 模型每次生成一个单词，直到endseq停止\n",
    "    for i in range(max_length-1):\n",
    "        # 把上一个循环为止生成的所有单词构成序列\n",
    "        intext = in_text.split()\n",
    "        sequence = tokenizer.texts_to_sequences([intext])[0]\n",
    "        print(sequence)\n",
    "        # 填充剩余部分，让序列长度为maxlength\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        # 输入模型，预测下一个单词\n",
    "        yhat = model.predict([sequence,photo], verbose=0)[0][i+1]\n",
    "        yhat[0] = 0            #去掉停止词\n",
    "        # 选择概率最高的为下一个单词\n",
    "        yhat = yhat.argmax()\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # 无法预测，停止\n",
    "        if word is None:\n",
    "            break\n",
    "        # 逐步添加单词\n",
    "        in_text +=' '+  word\n",
    "        # 遇到结尾词，停止\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text[10:len(in_text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1, 5]\n",
      "[1, 5, 10]\n",
      "[1, 5, 10, 8]\n",
      "[1, 5, 10, 8, 614]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'个 男人 正在 训狗 endseq'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 20\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    \n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    cnn_model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = cnn_model.predict(image, verbose=0)\n",
    "    feature=feature.reshape(1,49,512)\n",
    "    return feature\n",
    "\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('Data/Images/58368365_03ed3e5bdf.jpg')\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq', '男子', '站', '在', '山顶', '上', 'endseq']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions['58368365_03ed3e5bdf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
