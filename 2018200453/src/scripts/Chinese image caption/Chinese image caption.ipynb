{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "from os import listdir, mkdir\n",
    "from pickle import dump, load\n",
    "import zhon.hanzi\n",
    "import jieba\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, argmax\n",
    "\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, TimeDistributed, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from kulc.attention import ExternalAttentionRNNWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG提取图片特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c730c87c46c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data/Images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracted Features: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#def extract_features(directory):\n",
    "    # 去除最后一层，因为目的不是分类，而是特征抽取\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    model.summary()\n",
    "    \n",
    "    # 每张图片抽取特征\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # 将像素转存为数组形式\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # VGG的函数，预处理并预测\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        #存储图片特征\n",
    "        image_id = name.split('.')[0]\n",
    "        features[image_id] = feature\n",
    "        # print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "features,model2 = extract_features('Data/Images')\n",
    "print('Extracted Features: %d' % len(features))\n",
    "\n",
    "if not os.path.exists('MiddleFiles'):\n",
    "    mkdir('MiddleFiles')\n",
    "dump(features, open('MiddleFiles/features.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取文本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8091\n"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# 用字典存储标题-语句信息\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        # 以空格分隔\n",
    "        tokens = line.split()\n",
    "        # 第一个是图片标题，后面的是描述语句\n",
    "        image_id, image_desc = tokens[0], tokens[1]\n",
    "        if image_id[-1] != '0':\n",
    "            continue\n",
    "        # 去除.jpg的后缀\n",
    "        image_id = image_id.split('.')[0]\n",
    "        #以字典形式存储\n",
    "        mapping[image_id] = image_desc\n",
    "    return mapping\n",
    "\n",
    "#保存文件\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "doc = load_doc('Data/Captions/flickr8kzhc.caption.txt')\n",
    "\n",
    "# 加载描述语句\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d' % len(descriptions))\n",
    "\n",
    "# 保存文件\n",
    "if not os.path.exists('MiddleFiles'):\n",
    "    mkdir('MiddleFiles')\n",
    "save_descriptions(descriptions, 'MiddleFiles/descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.826 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptions_train: 6000\n",
      "Descriptions_val: 1000\n",
      "Descriptions: 7000\n",
      "Photos_train: 6000\n",
      "Photos_val: 1000\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集的图片名称\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    for line in doc.split('\\n'):\n",
    "        # 可能有空行\n",
    "        if not line:\n",
    "            continue\n",
    "        dataset.append(line)\n",
    "    return set(dataset)\n",
    "\n",
    "# 根据图片名称加载描述语句\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    punctuations = string.punctuation + zhon.hanzi.punctuation\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        tokens = line.split()\n",
    "        image_id, image_desc = tokens[0], tokens[1]\n",
    "        image_desc = jieba.cut(image_desc)\n",
    "        image_desc = [word for word in image_desc if word not in punctuations]\n",
    "        \n",
    "        # 描述语句首尾加开始/终止词，用于序列输入\n",
    "        image_desc.insert(0, 'startseq')\n",
    "        image_desc.append('endseq')\n",
    "        \n",
    "        # 跳过不在训练集中的图片\n",
    "        if image_id in dataset:\n",
    "            descriptions[image_id] = image_desc\n",
    "    return descriptions\n",
    "\n",
    "# 根据图片名称加载图片的特征向量\n",
    "def load_photo_features(filename, dataset):\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    features = {k: all_features[k].reshape(1, 49, 512) for k in dataset}\n",
    "    return features\n",
    "\n",
    "\n",
    "#加载图片名称\n",
    "image_ids_train = load_set('Data/Partition/flickr8ktrain.txt')\n",
    "image_ids_val = load_set('Data/Partition/flickr8kval.txt')\n",
    "image_ids_test = load_set('Data/Partition/flickr8ktest.txt')\n",
    "\n",
    "image_ids = set.union(image_ids_train,image_ids_val)\n",
    "\n",
    "print('Dataset: %d' % len(image_ids_train))\n",
    "\n",
    "#通过图片名称得到图片的描述语句\n",
    "descriptions_train = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids_train)\n",
    "print('Descriptions_train: %d' % len(descriptions_train))\n",
    "\n",
    "descriptions_val = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids_val)\n",
    "print('Descriptions_val: %d' % len(descriptions_val))\n",
    "\n",
    "descriptions = load_clean_descriptions('MiddleFiles/descriptions.txt', image_ids)\n",
    "print('Descriptions: %d' % len(descriptions))\n",
    "\n",
    "\n",
    "#通过图片名称得到图片的特征向量\n",
    "features_train = load_photo_features('MiddleFiles/features.pkl', image_ids_train)\n",
    "print('Photos_train: %d' % len(features_train))\n",
    "\n",
    "#通过图片名称得到图片的特征向量\n",
    "features_val = load_photo_features('MiddleFiles/features.pkl', image_ids_val)\n",
    "print('Photos_val: %d' % len(features_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建单词映射与序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 2260\n",
      "Description Length: 20\n"
     ]
    }
   ],
   "source": [
    "# 建立分词器tokenizer\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = list(descriptions.values())\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def one_hot_encode(data, MAXIMUM_CAPTION_LENGTH, n_classes):\n",
    "    result = np.zeros((len(data), MAXIMUM_CAPTION_LENGTH, n_classes))\n",
    "    for i, item in enumerate(data):\n",
    "        a = 0\n",
    "        for j, word in enumerate(item):\n",
    "            result[i, j, word] = 1.0\n",
    "            a = j\n",
    "        for k in range(a+1, MAXIMUM_CAPTION_LENGTH):\n",
    "            result[i, k, 0] = 1.0\n",
    "    return result\n",
    "\n",
    "def data_generator(batch_size, captions, get_image, tokenizer, max_length, vocab_size):\n",
    "    n_steps = int(np.ceil(len(captions) / batch_size))\n",
    "    while True:\n",
    "        for i in range(n_steps):\n",
    "            if i < n_steps - 1:\n",
    "                batch_indices = np.arange(i, i + batch_size)\n",
    "            else:\n",
    "                batch_indices = np.arange(64 * (n_steps - 1), len(captions))\n",
    "        \n",
    "            L = list(captions.keys())\n",
    "\n",
    "            batch_image_features = np.empty((len(batch_indices), 7*7, 512))\n",
    "            for i, j in enumerate(batch_indices):\n",
    "                batch_image_features[i] = get_image[L[j]].reshape((7*7, 512))\n",
    "\n",
    "            batch_captions = [captions[L[item]] for item in batch_indices]\n",
    "\n",
    "            input_captions = tokenizer.texts_to_sequences(batch_captions)\n",
    "            output_captions = tokenizer.texts_to_sequences(batch_captions)\n",
    "\n",
    "            input_captions = pad_sequences(input_captions, maxlen= max_length, padding='post')\n",
    "            output_captions = one_hot_encode(output_captions, max_length, vocab_size)\n",
    "       \n",
    "            batch_image_features = np.array(batch_image_features, dtype=np.float32)\n",
    "\n",
    "            x_data = [input_captions, batch_image_features]\n",
    "            y_data = output_captions\n",
    "\n",
    "            yield (x_data, y_data)\n",
    "\n",
    "\n",
    "#输出所有语句不同的单词总数\n",
    "tokenizer = create_tokenizer(descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "#输出单个语句最大长度\n",
    "max_length = max([len(desc) for desc in descriptions.values()])\n",
    "print('Description Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义与训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 49, 512)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 512)          0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 256)      578560      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 49, 512)      262656      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          131328      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          131328      lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "external_attention_rnn_wrapper_ [(None, 20, 256), (N 919296      embedding_3[0][0]                \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "                                                                 dense_8[0][0]                    \n",
      "                                                                 dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (TimeDistributed)        (None, 20, 2260)     580820      external_attention_rnn_wrapper_1[\n",
      "==================================================================================================\n",
      "Total params: 2,603,988\n",
      "Trainable params: 2,603,988\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 1.9645 - acc: 0.6628 - val_loss: 2.3296 - val_acc: 0.6648\n",
      "Epoch 2/30\n",
      "94/94 [==============================] - 6s 66ms/step - loss: 0.8537 - acc: 0.8122 - val_loss: 2.1392 - val_acc: 0.7101\n",
      "Epoch 3/30\n",
      "94/94 [==============================] - 6s 63ms/step - loss: 0.4003 - acc: 0.9383 - val_loss: 1.9605 - val_acc: 0.7489\n",
      "Epoch 4/30\n",
      "94/94 [==============================] - 6s 69ms/step - loss: 0.1839 - acc: 0.9830 - val_loss: 1.8586 - val_acc: 0.7636\n",
      "Epoch 5/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0933 - acc: 0.9948 - val_loss: 1.7868 - val_acc: 0.7727\n",
      "Epoch 6/30\n",
      "94/94 [==============================] - 6s 65ms/step - loss: 0.0500 - acc: 0.9976 - val_loss: 1.7305 - val_acc: 0.7831\n",
      "Epoch 7/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0298 - acc: 0.9984 - val_loss: 1.6961 - val_acc: 0.7899\n",
      "Epoch 8/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0198 - acc: 0.9990 - val_loss: 1.6783 - val_acc: 0.7887\n",
      "Epoch 9/30\n",
      "94/94 [==============================] - 6s 61ms/step - loss: 0.0148 - acc: 0.9993 - val_loss: 1.6508 - val_acc: 0.7978\n",
      "Epoch 10/30\n",
      "94/94 [==============================] - 5s 58ms/step - loss: 0.0114 - acc: 0.9995 - val_loss: 1.6301 - val_acc: 0.8002\n",
      "Epoch 11/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0085 - acc: 0.9997 - val_loss: 1.6193 - val_acc: 0.8039\n",
      "Epoch 12/30\n",
      "94/94 [==============================] - 6s 66ms/step - loss: 0.0078 - acc: 0.9996 - val_loss: 1.5945 - val_acc: 0.8067\n",
      "Epoch 13/30\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.0061 - acc: 0.9997 - val_loss: 1.5908 - val_acc: 0.8114\n",
      "Epoch 14/30\n",
      "94/94 [==============================] - 6s 68ms/step - loss: 0.0051 - acc: 0.9997 - val_loss: 1.5888 - val_acc: 0.8138\n",
      "Epoch 15/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0044 - acc: 0.9998 - val_loss: 1.5877 - val_acc: 0.8121\n",
      "Epoch 16/30\n",
      "94/94 [==============================] - 6s 60ms/step - loss: 0.0038 - acc: 0.9997 - val_loss: 1.5769 - val_acc: 0.8123\n",
      "Epoch 17/30\n",
      "94/94 [==============================] - 6s 63ms/step - loss: 0.0034 - acc: 0.9997 - val_loss: 1.5662 - val_acc: 0.8156\n",
      "Epoch 18/30\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.0030 - acc: 0.9996 - val_loss: 1.5610 - val_acc: 0.8161\n",
      "Epoch 19/30\n",
      "94/94 [==============================] - 6s 63ms/step - loss: 0.0027 - acc: 0.9997 - val_loss: 1.5572 - val_acc: 0.8174\n",
      "Epoch 20/30\n",
      "94/94 [==============================] - 7s 70ms/step - loss: 0.0025 - acc: 0.9997 - val_loss: 1.5492 - val_acc: 0.8191\n",
      "Epoch 21/30\n",
      "94/94 [==============================] - 6s 59ms/step - loss: 0.0022 - acc: 0.9997 - val_loss: 1.5540 - val_acc: 0.8209\n",
      "Epoch 22/30\n",
      "94/94 [==============================] - 6s 67ms/step - loss: 0.0021 - acc: 0.9997 - val_loss: 1.5361 - val_acc: 0.8202\n",
      "Epoch 23/30\n",
      "94/94 [==============================] - 5s 57ms/step - loss: 0.0019 - acc: 0.9998 - val_loss: 1.5433 - val_acc: 0.8206\n",
      "Epoch 24/30\n",
      "94/94 [==============================] - 6s 61ms/step - loss: 0.0017 - acc: 0.9998 - val_loss: 1.5361 - val_acc: 0.8215\n",
      "Epoch 25/30\n",
      "94/94 [==============================] - 6s 64ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 1.5390 - val_acc: 0.8169\n",
      "Epoch 26/30\n",
      "94/94 [==============================] - 5s 57ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 1.5404 - val_acc: 0.8206\n",
      "Epoch 27/30\n",
      "94/94 [==============================] - 6s 67ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 1.5290 - val_acc: 0.8235\n",
      "Epoch 28/30\n",
      "94/94 [==============================] - 6s 63ms/step - loss: 0.0012 - acc: 0.9999 - val_loss: 1.5227 - val_acc: 0.8228\n",
      "Epoch 29/30\n",
      "94/94 [==============================] - 6s 64ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 1.5043 - val_acc: 0.8255\n",
      "Epoch 30/30\n",
      "94/94 [==============================] - 5s 58ms/step - loss: 9.4393e-04 - acc: 0.9999 - val_loss: 1.5152 - val_acc: 0.8222\n"
     ]
    }
   ],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # 以下是图片特征提取层\n",
    "    inputs1 = Input(shape=(49, 512))\n",
    "    image_features = TimeDistributed(Dense(512, activation=\"relu\"))(inputs1)\n",
    "    \n",
    "    # 以下是文本特征提取层\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256)(inputs2)\n",
    "    \n",
    "    #以下是attention层\n",
    "    averaged_image_features = Lambda(lambda x: K.mean(x, axis=1))\n",
    "    averaged_image_features = averaged_image_features(inputs1)\n",
    "    initial_state_h = Dense(256)(averaged_image_features)\n",
    "    initial_state_c = Dense(256)(averaged_image_features)\n",
    "    \n",
    "    encoder = LSTM(256, return_sequences=True, return_state=True, recurrent_dropout=0.1)\n",
    "    attented_encoder = ExternalAttentionRNNWrapper(encoder, return_attention=True)\n",
    "    output = TimeDistributed(Dense(vocab_size, activation=\"softmax\"), name=\"output\")\n",
    "    attented_encoder_training_data, _, _ , _= attented_encoder([se1,image_features], initial_state=[initial_state_h, initial_state_c])\n",
    "    training_output_data = output(attented_encoder_training_data)\n",
    "    \n",
    "    model = Model(inputs=[inputs2, inputs1], outputs=training_output_data)\n",
    "    model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "    \n",
    "    # 模型描述与绘制\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "\n",
    "# 保存模型\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=True)\n",
    "\n",
    "steps = len(descriptions)\n",
    "batch_size = 64\n",
    "\n",
    "# create the data generator\n",
    "generator = data_generator(batch_size, descriptions_train, features_train, tokenizer, max_length, vocab_size)\n",
    "validation_generator = data_generator(batch_size, descriptions_val, features_val, tokenizer, max_length, vocab_size)\n",
    "\n",
    "# fit for one epoch\n",
    "history = model.fit_generator(generator, epochs=30, validation_data=validation_generator,\n",
    "                              validation_steps=int(np.ceil(len(descriptions_val) / batch_size)),\n",
    "                              steps_per_epoch=int(np.ceil(len(descriptions_train) / batch_size)), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字映射到单词\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# 把模型预测转换成语句\n",
    "def generate_desc(model, tokenizer, photo, max_length): ## 需要修改!!!!!\n",
    "    in_text = 'startseq'\n",
    "    # 模型每次生成一个单词，直到endseq停止\n",
    "    for i in range(max_length-1):\n",
    "        # 把上一个循环为止生成的所有单词构成序列\n",
    "        intext = in_text.split()\n",
    "        sequence = tokenizer.texts_to_sequences([intext])[0]\n",
    "        # 填充剩余部分，让序列长度为maxlength\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
    "        # 输入模型，预测下一个单词\n",
    "        yhat = model.predict([sequence,photo], verbose=0)[0][i+1]\n",
    "        yhat[0] = 0            #去掉停止词\n",
    "        # 选择概率最高的为下一个单词\n",
    "        yhat = yhat.argmax()\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # 无法预测，停止\n",
    "        if word is None:\n",
    "            break\n",
    "        # 逐步添加单词\n",
    "        in_text +=' '+  word\n",
    "        # 遇到结尾词，停止\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text[10:len(in_text)]\n",
    "\n",
    "def inference(image_features, plot_attention):\n",
    "    image_features = np.array([image_features])\n",
    "    state_h, state_c = initial_state_inference_model.predict(image_features)\n",
    "\n",
    "    caption = [le.transform_word(\"<START>\")]\n",
    "    attentions = []\n",
    "\n",
    "    current_word = None\n",
    "    for t in range(MAXIMUM_CAPTION_LENGTH):\n",
    "        caption_array = np.array(caption).reshape(1, -1)\n",
    "        output, state_h, state_c, attention = inference_model.predict([image_features, caption_array, state_h, state_c])\n",
    "        attentions.append(attention[0, -1].reshape((14, 14)))\n",
    "\n",
    "        current_word = np.argmax(output[0, -1])\n",
    "        caption.append(current_word)\n",
    "\n",
    "        if current_word == le.transform_word(\"<STOP>\"):\n",
    "            break\n",
    "    sentence = [le._index_word_map[i] for i in caption[1:]]\n",
    "\n",
    "    if plot_attention:\n",
    "        print(len(attentions))\n",
    "        x = int(np.sqrt(len(attentions)))\n",
    "        y = int(np.ceil(len(attentions) / x))\n",
    "        _, axes = plt.subplots(y, x, sharex=\"col\", sharey=\"row\")\n",
    "        axes = axes.flatten()\n",
    "        for i in range(len(attentions)):\n",
    "            atn = skimage.transform.pyramid_expand(attentions[i], upscale=16, sigma=20)\n",
    "            axes[i].set_title(sentence[i])\n",
    "            axes[i].imshow(atn, cmap=\"gray\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    return \" \".join(sentence) + \" ({0})\".format(len(caption)-1)\n",
    "\n",
    "\n",
    "\n",
    "#模型BLEU分数计算\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    cout=0\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # yhat为模型的输出语句\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # references使实际的描述语句\n",
    "        references = [d.split() for d in desc_list]\n",
    "        \n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "        \n",
    "    # 计算BLEU分数\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "#evaluate_model(model, descriptions, features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenizer\n",
    "dump(tokenizer, open('MiddleFiles/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 有 有 有 有 小男孩 小男孩 小男孩 玩水 玩 玩 endseq'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tokenizer\n",
    "# tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 20\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    \n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    cnn_model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    image = preprocess_input(image)\n",
    "    feature = cnn_model.predict(image, verbose=0)\n",
    "    feature=feature.reshape(1,49,512)\n",
    "    return feature\n",
    "\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('Data/Images/2187222896_c206d63396.jpg')\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['startseq', '穿着', '红色', '衣服', '的', '男孩', 'endseq']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions_train['2187222896_c206d63396']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
