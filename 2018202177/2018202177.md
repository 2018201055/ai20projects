# Task 9 HumanEye 终期报告

GroupID: 22  StuID: 2018202177  Name: 官佳薇



**项目概述：对给定图像，生成文字语句对图像内容产生描述。**

在经典图像标注算法 ***Show, Attend and Tell*** 的原理支撑下，完成数据预处理、CNN图像特征抽取（Encoder）、Attention和LSTM框架实现。 

项目使用 `PyTorch 1.0`和  `python3.7`



## 目录

* [Objective](#objective)
* [Data Preprocessing](#data-preprocessing)
* [Inputs to model](#inputs-to-model)
* [Encoder](#encoder)
* [Decoder](#decoder)
* [Attention](#attention)
* [Training](#training)
* [Validation](#validation)
* [模型训练](#----)
* [结果测试](#----)
* [Summary](#summary)



## Objective

在本项目中，我们将首先使用CNN和RNN建立图像标注模型。

以***Show, Attend and Tell*** 算法为基础，使用CNN提取图像特征。

利用RNN生成文字序列，并利用**注意力机制**实现对图像不同区域加权训练。



## Data Preprocessing



#### 数据集描述

数据集采用经典的图像标注数据Fliker8k, Fliker30k, MS COCO。考虑计算资源，采用Fliker8k进行训练和测试。

数据集包括图片数据（获取方式见src/datasets/数据说明.md），及相应的图像文字标注数据（src/datasets/dataset_flickr8k.json)。



#### 数据处理

- **Train & Val & Test Split.**  utils.py文件实现了数据预处理过程，根据Flickr8k中各图像的描述将图像划分成训练、验证和测试集。

- **Word Embedding.**  使用collection库的`Counter()`计数器，统计训练集图像标注中出现的所有单词词频，对单词出现次数大于一定阈值（实现中暂定为5）的单词进行保留。为单词顺序编号，并加入四个特殊词`<unk> <start> <end> <pad>`，创建单词到编号的`wordmap`。
  1. `<unk>`  未知词，用于代替所有未出现在词表中的词
  2. `<start>` Caption的开始标志
  3. `<end>`  Caption的结束标志
  4. `<pad>`  填充，将所有句子填充成相同长度

- **Image Standardization.** 对于一张三通道的模型，我们需要通过Encoder得到一个高维的特征表达，我们使用在ImageNet上预训练模型 `ResNet-50` 作为Encoder，对于图片输入需要做一些处理：
  1. 将训练集、验证集、测试集图片resize到(3,256,256)，生成3各HDF5文件。
  2. 创建三个对应的JSON文件，对应训练集，验证集和测试集的字幕，字母顺序和HDF5文件中图片顺序一致。每张图片有5个字幕，每句字幕都被统一到52的长度。

- **Caption Encoding for Training&Val Dataset.**  对于训练集和验证集中已知的图像标注语句产生句子的Embedding。

  每个Caption的组成包括：

  	1. `<start>`
   	2. 每个单词的wordmap编号，若单词不在wordmap中，使用`<unk>`代替
   	3. `<end>`
   	4. 填充`<pad>`直至最大句子长度。

  

## Inputs to model

模型包含有 3 个输入：图片、图片描述、描述句子的长度。



## Encoder

编码器的主要作用是将一张输入的3通道图片编码成固定格式，作为对原始图片的特征表述。考虑到是对图像进行特征抽取，故自然选择CNN作为编码器。目前在实现中我们选择使用torchvision中与训练的 **ResNet-50** 模型，并微调其中部分参数来提升模型性能。该模型通过逐渐的卷积和池化操作，使得图片特征越来越小，通道数越来越多，以表达语义。

CNN模型常用来进行图像识别，而此处我们仅需抽取特征，故删除最后两层（池化层和全连接层）。增加`nn.AdaptiveAvgPool2d()`函数，将特征转换到固定大小。参考对CNN的参数调整，在Encoder中加入了`freeze_params()`方法，通过该函数控制是否对Encoder中的参数进行微调，最后的特征形状为14×14×2048（2048为通道数）。

```python
class Encoder(nn.Module):
    def __init__(self, encoded_image_size=14):
        super(Encoder, self).__init__()
        self.enc_img_size = encoded_image_size
        cnn_ext = torchvision.models.resnet50(pretrained = True)  # 使用预训练的 resnet-50
        modules = list(cnn_ext.children())[:-2]  # 去掉网络中的最后两层
        self.cnn_ext = nn.Sequential(*modules)  # 定义好 encoder
        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))  # 将输出改变到指定的大小

    def forward(self, img):
        out = self.cnn_ext(img)  # [bs, 2048, 8, 8]
        out = self.adaptive_pool(out)  # [bs, 2048, enc_img_size, enc_img_size]
        out = out.permute(0, 2, 3, 1)  # [bs, enc_img_size, enc_img_size, 2048]
        return out

    def freeze_params(self, freeze):
        for p in self.cnn_ext.parameters():
            p.requires_grad = False
        for c in list(self.cnn_ext.children())[5:]:
            for p in c.parameters():
                p.requires_grad = (not freeze)
```

在`test.py`中，我们使用数据集中一张图片对Encoder的正确性进行了测试，正确产生了14×14×2048维度的图像特征。



## Decoder

Decoder的主要作用是通过编码之后的图像，逐次生成一句图像描述。因为生成的字幕是一段序列，需要使用循环神经网络，这里使用LSTMcell。解码器的实现在`models.py`中的DecoderWithAttention中。

#### 定义&初始化Decoder网络层

- **LSTEMCell.** 定义LSTMCell作为Decoder中的序列模块。

- **隐藏层.** 在RNN的实现中，我们使用了LSTMCell，模型需要传入初始的hidden state和cell state，在一般的LSTM中，我们可以将其初始化为0，但现在我们有Encoder的输出，考虑利用该输出作为LSTM的hidden state 和 cell state，以提升性能。

- **线性层、激活函数sigmoid、全连接层**

- **词嵌入表示**. 根据数据预处理中得到的word_map，将每个单词用word_map中的数字代表，即整个描述语句为一个向量，其中每个数字代表一个词。以该特征向量为基础，调用nn.Embedding方法获得词嵌入表示。

  在Decoder中，一个关键问题是表示出每个单词的词向量，我们使用word embedding来实现单词低维向量的快速输出。 在pytorch中实现word embedding是通过一个函数实现的：`nn.Embedding()`。通过输入每个单词的数字表示，获得对应的word embedding。

- **Attention机制**

  

#### 不含Attention的LSTM流程：

我们将编码之后的特征做一个全局平均池化，然后通过仿射变换之后作为隐含状态 $h_0$ 输入到 LSTM 当中，生成一个单词，同时生成下一步的隐含状态 $h_1$，接着该隐含状态和当前预测的单词作为下一次的输入，再一次输入到 LSTM 当中得到下一步的输出，通过不断的生成，直到最后模型输出一个结束标志 `<end>`，就终止模型的继续生成。

![decoderWithoutAttention](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/decoderWithoutAttention.png?raw=true)

***

#### 含Attention的LSTM流程：

加入attention机制，序列中生成一个单词时，模型需要学会每一个单词要关注图片中的哪一个位置。解码器不再是对特征做全局平均，而是先得到一个注意力矩阵，通过注意力矩阵与特征相乘，让模型知道应该注意哪些像素点，再输入Decoder中生成单词。

![Decoder With Attention](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/decoderWithAttention.png?raw=true)



#### **核心流程**

1. **图像特征抽取**  利用Encoder提取图像特征

2. **初始化LSTM隐藏层和cell层**

   LSTMCell模型需要的参数有：初始隐藏层$h_0$ $c_0$，标注单词，注意力。因此在Decoder循环开始前需要获取初始隐藏层。在一般的 LSTM 中，我们可以将其初始化为 0，但是现在我们有 Encoder 的输出，所以我们利用 `encoder_out` ，对 `encoder_out` 在pixcel维度求平均，通过线性映射得到隐藏层。

   ```python
   def init_hidden_state(self, encoder_out):
           # 对所有的 pxiel 求平均
           mean_encoder_out = encoder_out.mean(dim=1)
           # 线性映射分别得到 hidden state 和 cell state
           h = self.init_h(mean_encoder_out)
           c = self.init_c(mean_encoder_out)
           return h, c
   ```

3. **对输入的batch个Caption语句按长度降序排序**  通过这个排序我们可以每次都对有效的序列进行建模，不用处理 `<pad>`数据。

   ``` python
   caption_lens, sort_idx = caption_lens.squeeze(1).sort(dim=0, descending=True)
   encoder_out = encoder_out[sort_idx]
   encoded_captions = encoded_captions[sort_idx]
   ```

4. **利用LSTM隐藏层信息求得注意力矩阵** 

   公式：

   ![Fomula.JPG](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/Fomula.JPG?raw=true)

   ```python
   attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],  h[:batch_size_t])
   gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # 根据公式计算 soft attention 结果 gate过滤
   attention_weighted_encoding = gate * attention_weighted_encoding
   ```

5. **LSTMCell 模块训练** 

   将结合注意力的特征图像与embedding拼接在一起，结合隐藏层输入到LSTMCell中，得到新的hidden state(output), cell state。hidden state输出并进行softmax，得到预测出词表中每个单词的概率。

   ```python
   h, c = self.decode_step(
                   torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
                   (h[:batch_size_t], c[:batch_size_t]))
   ```



## Attention

对于一张图片，当描述到某一个单词的时候，比如 `a man` ，那么我们会注意到图片中人的区域，所以这个时候我们更加注意有人区域的像素点，而当描述到 `carriage` 的时候，我们又会注意到有马车的区域，所以每一次当我们要注意某一个区域的时候，我们需要考虑我们的句子描述到了哪里。而句子的描述信息在LSTM的隐藏层中包含，故每一次计算注意力矩阵的时候，都需要使用 LSTM 的隐含状态，以确定注意力。



![Attention](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/att.png?raw=true)

这里我们使用 soft attention，也就是每一步的注意力矩阵所有的元素求和为1。

![soft attention](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/doublystochastic.png?raw=true)

使用 `softmax` 实现soft attention，同时作为一个概率的解释。即再生成下一个单词的时候，哪个部分被注意到的概率最大。

***

Attention机制实现相对容易，仅包含先行层和激活函数。

难点在于对模型矩阵运算维度的把握，实现过程中多次在向量维度方面出现bug。

#### Attention定义：

```python
    def __init__(self, encoder_dim, decoder_dim, attention_dim):
        super(AttentionModule, self).__init__()
        self.encoder_att = nn.Linear(encoder_dim, attention_dim) 
        self.decoder_att = nn.Linear(decoder_dim, attention_dim) 
        self.full_att = nn.Linear(attention_dim, 1)  
        self.relu = nn.ReLU()  
        self.softmax = nn.Softmax(dim=1)  
```

#### Attention实现具体步骤：

- 将拉平的 `encoder_out (bs, 14*14, encoder_dim)` 线性映射到 `(bs, 14*14, attention_dim)` 得到 `att1`
- 将 decoder 中的 `decoder_hidden (bs, decoder_dim)` 线性映射到 `(bs, attention_dim)` 得到 `att2`
- 计算 `att = Linear(ReLU(att1 + att2))` 得到 `(bs, 14*14, 1)`
- 使用 `softmax` 得到注意力矩阵 `alpha (bs, 14*14)`，并将`alpha`和`encoder_out`左加权平均得到注意力机制的输出。

#### forward

```python
    def forward(self, encoder_out, decoder_hidden):
        """
        注意力机制的前向传播过程
        :param encoder_out: 提取的图片特征，大小是 (bs, num_pixels, encoder_dim)
        :param decoder_hidden: 前一步的解码输出，大小是 (bs, decoder_dim)
        :return: 注意力编码的权重矩阵
        """
        att1 = self.encoder_att(encoder_out)  
        att2 = self.decoder_att(decoder_hidden)  
        att2 = att2.unsqueeze(1)  
        att = att1 + att2  
        att = self.relu(att)  
        att = self.full_att(att)  
        att = att.squeeze(2) 
        alpha = self.softmax(att) 
        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(1)  
        return attention_weighted_encoding, alpha
```



## Training

模型训练代码在`train.py`中，模型训练使用`Adam`优化器。

分批次进行训练，按batch_size读入图像和对应标注数据。

#### Loss Function

由于模型产生的是一个单词序列，使用**交叉熵损失函数**计算误差。在计算交叉熵损失的时候，为了避免在 `<pad>` 上引入的额外计算，PyTorch 中提供了一个非常方便的函数来解决这个问题 `pack_padded_sequence()`，这个函数会将一个 batch 的不同长度的序列拉平在一起，同时忽略其中的 `<pad>` ，从 batch 数据中提取有效的序列长度，可以节约 LSTM 的计算开销。

#### Teacher Forcing

在LSTM的每一个时间步，使用真实caption作为Decoder的输入，使得模型更快收敛。

#### Training Tricks

- 默认最大训练迭代次数epoch=120。但如若训练过程中连续8各epoch的性能没有改善，则降低学习率，继续训练；如若训练过程中连续20各epoch没有性能改善，则直接停止训练。
- Checkpoints. 阶段性设置checkpoint保存训练模型。
- 为防止梯度爆炸现象，使用nn.utils.clip_grad_value_方法进行梯度裁剪



## Validation

每训练一个epoch后，都使用当前训练得到的最好模型在验证集上进行测试，测试过程同Training过程基本一致。



## 模型训练

目前考虑到计算资源问题，仅在Flickr8k数据集上进行了模型训练。共训练了47个epochs，在validation集上的验证准确率可达到70%。

![Finish-flickr8k.JPG](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/Finish-flickr8k.JPG?raw=true)



## 结果测试

代码在`inference.py`中。

#### 字幕注意力热力图可视化

在Decoder的每一次迭代预测单词的过程中，均经过了Attention机制求得每个pixcel的权重alpha，保存alpha以便对每个单词的注意力进行可视化。首先将alpha与输入图像大小想对齐，使用高斯滤波对alpha的热力图效果进行平滑处理，覆盖在原始图像上。

#### 输出结果

随机输入图像测试结果，并将字幕及其注意力热力图进行可视化，得到结果如下。

#### 测试1

<img src="https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/19212715_20476497a3.jpg?raw=true" alt="19212715_20476497a3.jpg" style="zoom:67%;" />

​						输出：`a person kayaking in the ocean.`

![image-20201121105700138.png](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/image-20201121105700138.png?raw=true)

***

#### 测试2

<img src="https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/42637986_135a9786a6.jpg?raw=true" alt="42637986_135a9786a6.jpg" style="zoom: 67%;" />

​						输出：`a person kayaking in the ocean.`

![pic2.jpg](https://github.com/Guan-JW/ai20projects/blob/master/2018202177/pics/pic2.jpg?raw=true)





## Summary

针对Human Eye任务，我们依据 ***Show, Attend and Tell*** 提供的算法，构建了CNN提取特征、LSTM构建图像标注语句的模型。在训练了47个epoch后模型达到收敛，最终得到70%的准确率。在实际图片上测试效果，对于内容非常复杂的图像，文字标注的结果不够准确。但对于主体明确的图像，输出结果较好。