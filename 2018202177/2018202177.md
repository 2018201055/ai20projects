# Task 9 HumanEye 中期报告

GroupID: 22  StuID: 2018202177  Name: 官佳薇



项目概述：对给定图像，生成文字语句对图像内容产生描述。

目前，我们在经典图像标注算法Show, Attend and Tell的原理支撑下，完成了数据预处理、CNN图像特征抽取（Encoder）、Attention和LSTM框架实现。

项目使用 `PyTorch 1.0`和  `python3.7`



## Objective

在本项目中，我们将首先使用CNN和RNN建立图像标注模型。

1. 以***Show, Attend and Tell*** 算法为基础，使用CNN提取图像特征，同时利用RNN生成文字序列，并利用**注意力机制**实现对图像不同区域加权训练。

   `Addition`

2. 以***Spatial and Channel-wise Attention in Convolutional Networks***论文中的算法为基础，利用CNN的空间性、多通道和多层级性质，对Encoder的特征提取能力进行增强。

3. 以***Bottom-UP and Top-Down Attention***算法为基础，在实现图像物体识别和检测的基础上，对图像中识别出的部分区域进行特征提取，对模型加强针对性。

目前我们实现了第一阶段中的CNN特征提取和RNN框架实现。



## Data Preprocessing

#### 数据集描述

数据集采用经典的图像标注数据Fliker8k, Fliker30k, MS COCO。考虑计算资源，目前采用Fliker8k进行测试。

数据集包括图片数据（获取方式见src/datasets/数据说明.md），及相应的图像文字标注数据（src/datasets/dataset_flickr8k.json)。

#### 图像预处理

​	data_process.py文件实现了数据预处理过程，将Fliker8k Dataset划分成训练集和验证集。

​	读入图片并resize到256×256大小，即每个图像的形状为(3, 256, 256)，根据dataset_flicker8k.json文件中对图像的描述（包含该图像属于训练集、测试集还是验证集），对图像和文字描述进行分类。将图像存入3个HDF5文件，将文字描述存入3个Json文件，对应训练集、测试集、验证集。每张图片保留5句描述，每句描述被补长或缩短到52的长度（包含<start>和<end>）。

​	生成3个对应的json文件，分别保存训练集、测试集、验证集中每个图像的实际文字描述长度。

​	使用collections库的Counter()计数器，对出现的单词进行词频统计，对单词出现次数大于一定阈值（实现中暂定为5）的单词进行保留，并进行数字编码，生成.json文件。

## Encoder

编码器的主要作用是将一张输入的3通道图片编码成固定格式，作为对原始图片的特征表述。考虑到是对图像进行特征抽取，故自然选择CNN作为编码器。目前在实现中我们选择使用torchvision中与训练的ResNet-50模型，并微调其中部分参数来提升模型性能。该模型通过逐渐的卷积和池化操作，使得图片特征越来越小，通道数越来越多，以表达语义。

CNN模型常用来进行图像识别，而此处我们仅需抽取特征，故删除最后两层（池化层和全连接层）。增加`nn.AdaptiveAvgPool2d()`函数，将特征转换到固定大小。参考对CNN的参数调整，在Encoder中加入了`freeze_params()`方法，通过该函数控制是否对Encoder中的参数进行微调，最后的特征形状为14×14×2048（2048为通道数）。

```python
class Encoder(nn.Module):
    def __init__(self, encoded_image_size=14):
        super(Encoder, self).__init__()
        self.enc_img_size = encoded_image_size
        cnn_ext = torchvision.models.resnet50(pretrained = True)  # 使用预训练的 resnet-50
        modules = list(cnn_ext.children())[:-2]  # 去掉网络中的最后两层
        self.cnn_ext = nn.Sequential(*modules)  # 定义好 encoder
        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))  # 将输出改变到指定的大小

    def forward(self, img):
        out = self.cnn_ext(img)  # [bs, 2048, 8, 8]
        out = self.adaptive_pool(out)  # [bs, 2048, enc_img_size, enc_img_size]
        out = out.permute(0, 2, 3, 1)  # [bs, enc_img_size, enc_img_size, 2048]
        return out

    def freeze_params(self, freeze):
        for p in self.cnn_ext.parameters():
            p.requires_grad = False
        for c in list(self.cnn_ext.children())[5:]:
            for p in c.parameters():
                p.requires_grad = (not freeze)
```

在test.py中，我们使用数据集中一张图片对Encoder的正确性进行了测试，正确产生了14×14×2048维度的图像特征。



## Attention

注意力机制的实现代码是models.py中的`AttentionModule`，注意力机制的网络结构比较简单，仅包含一些线性层和激活函数，目前我们实现了这些层的基本定义和Attention机制的主体框架，尚未实现其计算过程。



## Decoder

解码器的实现代码在models.py中的DecoderWithAttention，目前我们实现了不含Attention机制的Decoder主体框架，包括定义decoder中需要的网络层，初始隐藏层的初始化，和基本的向前传播机制forward。

 	1. 隐藏层初始化：在RNN的实现中，我们使用了LSTMCell，模型需要传入初始的hidden state和cell state，在一般的LSTM中，我们可以将其初始化为0，但现在我们有Encoder的输出，考虑利用该输出作为LSTM的hidden state 和 cell state，以提升性能。

	2. 词嵌入表示：根据数据预处理中得到的word_map，将每个单词用word_map中的数字代表，即整个描述语句为一个向量，其中每个数字代表一个词。以该特征向量为基础，调用nn.Embedding方法获得词嵌入表示。
 	3. 对输入图像和词嵌入向量按照每个图像的描述语句长度进行级那个序排列，给更长的语句更高的权重，对它们处理更多次，对更短的语句更低的权重，对它们处理较少次。按时间步取小批次进行LSTM训练。



## Future

后一阶段我们将进一步完善Encoder, Attention, Decoder三大模块，尝试在小数据集上进行模型训练和预测。若时间允许，我们会进一步尝试图像物体识别，提升模型效果。