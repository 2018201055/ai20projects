{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit4c03300bedca44f8b0013abe02048abc",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def sinkhorn_forward(C,mu,nu,epsilon,max_iter):\n",
    "    print(C)\n",
    "    bs,n,k_ = C.size()\n",
    "    v = torch.ones([bs,1,k_])/(k_)\n",
    "    G = torch.exp(-C/epsilon)\n",
    "    if torch.cuda.is_available():\n",
    "        v = v.cuda()\n",
    "    for i in range(max_iter):\n",
    "        u = mu/(G * v).sum(-1,keepdim=True)\n",
    "        v = nu/(G * u).sum(-2,keepdim=True)\n",
    "    Gamma = u * G * v\n",
    "    return Gamma\n",
    "\n",
    "def sinkhorn_forward_stablized(C,mu,nu,epsilon,max_iter):\n",
    "    bs,n,k_ = C.size()\n",
    "    k = k_-1\n",
    "    f = torch.zeros([bs,n,1])\n",
    "    g = torch.zeros([bs,1,k+1])\n",
    "    if torch.cuda.is_available():\n",
    "        f=f.cuda()\n",
    "        g=g.cuda()\n",
    "        \n",
    "    epsilon_log_mu = epsilon * torch.log(mu)\n",
    "    epsilon_log_nu = epsilon * torch.log(nu)\n",
    "\n",
    "    def min_epsilon_row(Z,epsilon):\n",
    "        return -epsilon * torch.logsumexp((-Z)/epsilon,-1,keepdim=True)\n",
    "    def min_epsilon_col(Z,epsilon):\n",
    "        return -epsilon * torch.logsumexp((-Z)/epsilon,-2,keepdim=True)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        f = min_epsilon_row(C-g,epsilon) + epsilon_log_mu\n",
    "        g = min_epsilon_col(C-f,epsilon) + epsilon_log_nu\n",
    "\n",
    "    Gamma=torch.exp((-C + f + g)/epsilon)\n",
    "    return Gamma\n",
    "\n",
    "def sinkhorn_backward(grad_output_Gamma,Gamma,mu,nu,epsilon):\n",
    "    nu_ = nu[:,:,:-1]\n",
    "    Gamma_ = Gamma[:,:,:-1]\n",
    "    bs,n,k_ = Gamma.size()\n",
    "    inv_mu = 1./(mu.view([1,-1]))   #[1,n]\n",
    "    Kappa = torch.diag_embed(nu_.squeeze(-2))\\\n",
    "        -torch.matmul(Gamma_.transpose(-1,-2) * inv_mu.unsqueeze(-2),Gamma_)    #[bs,k,k]\n",
    "\n",
    "    inv_Kappa = torch.inverse(Kappa)  #[bs,k,k]\n",
    "    Gamma_mu = inv_mu.unsqueeze(-1)*Gamma_\n",
    "    L = Gamma_mu.matmul(inv_Kappa)  #[bs,n,k]\n",
    "    G1 = grad_output_Gamma * Gamma  #[bs,n,k+1]\n",
    "    g1 = G1.sum(-1)\n",
    "    G21 = (g1 * inv_mu).unsqueeze(-1) * Gamma   #[bs,n,k+1]\n",
    "    g1_L = g1.unsqueeze(-2).matmul(L)   #[bs,1,k]\n",
    "    G22 = g1_L.matmul(Gamma_mu.transpose(-1,-2)).transpose(-1,-2) * Gamma   #[bs,n,k+1]\n",
    "    G23 = -F.pad(g1_L,pad=(0,1),mode='constant',value=0) * Gamma  #[bs,n,k+1]\n",
    "    G2 = G21 + G22 + G23    #[bs,n,k+1]\n",
    "    \n",
    "    del g1,G21,G22,G23,Gamma_mu\n",
    "    \n",
    "    g2 = G1.sum(-2).unsqueeze(-1) #[bs,k+1,1]\n",
    "    g2 = g2[:,:-1,:]              #[bs,k,1]\n",
    "    G31 = -L.matmul(g2) * Gamma     #[bs,n,k+1]\n",
    "    G32 = F.pad(inv_Kappa.matmul(g2).transpose(-1,-2),pad=(0,1),mode='constant',value=0) * Gamma    #[bs,n,k+1]G3=G31+G32#[bs,n,k+1]\n",
    "    grad_C = (-G1+G2+G3)/epsilon#[bs,n,k+1]\n",
    "    return grad_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx,C,mu,nu,epsilon,max_iter):\n",
    "        with torch.no_grad():\n",
    "            if epsilon>1e-2:\n",
    "                Gamma = sinkhorn_forward(C,mu,nu,epsilon,max_iter)\n",
    "                if bool(torch.any(Gamma != Gamma)):\n",
    "                    print('Nan appeared in Gamma,re-computing...')\n",
    "                    Gamma = sinkhorn_forward_stablized(C,mu,nu,epsilon,max_iter)\n",
    "            else:\n",
    "                Gamma = sinkhorn_forward_stablized(C,mu,nu,epsilon,max_iter)\n",
    "            \n",
    "            ctx.save_for_backward(mu,nu,Gamma)\n",
    "            ctx.epsilon = epsilon\n",
    "        return Gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx,grad_output_Gamma):\n",
    "        epsilon = ctx.epsilon\n",
    "        mu,nu,Gamma = ctx.saved_tensors\n",
    "        #mu[1,n,1]\n",
    "        #nu[1,1,k+1]\n",
    "        #Gamma[bs,n,k+1]\n",
    "        with torch.no_grad():\n",
    "            grad_C = sinkhorn_backward(grad_output_Gamma,Gamma,mu,nu,epsilon)\n",
    "        return grad_C,None,None,None,None\n",
    "\n",
    "class TopK_custom(torch.nn.Module):\n",
    "    def __init__(self,k,epsilon = 0.1,max_iter = 200):\n",
    "        super(TopK_custom,self).__init__()\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.anchors = torch.FloatTensor([k-i for i in range(k+1)]).view([1,1,k+1])\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.anchors = self.anchors.cuda()\n",
    "\n",
    "    def forward(self,scores):\n",
    "        bs,n = scores.size()\n",
    "        scores = scores.view([bs,n,1])\n",
    "        # find the -inf value and replace it with the minimum value except -inf\n",
    "        scores_ = scores.clone().detach()\n",
    "        max_scores = torch.max(scores_).detach()\n",
    "        scores_[scores_==float('-inf')] = float('inf')\n",
    "        min_scores = torch.min(scores_).detach()\n",
    "        filled_value = min_scores-(max_scores-min_scores)\n",
    "        mask = scores==float('-inf')\n",
    "        scores = scores.masked_fill(mask,filled_value)\n",
    "        C = (scores-self.anchors)**2\n",
    "        C = C/(C.max().detach())\n",
    "        mu = torch.ones([1,n,1],requires_grad = False)/n\n",
    "        nu = [1./n for _ in range(self.k)]\n",
    "        nu.append((n-self.k)/n)\n",
    "        nu = torch.FloatTensor(nu).view([1,1,self.k+1])\n",
    "        if torch.cuda.is_available():\n",
    "            mu = mu.cuda()\n",
    "            nu = nu.cuda()\n",
    "        \n",
    "        Gamma = TopKFunc.apply(C,mu,nu,self.epsilon,self.max_iter)\n",
    "        A = Gamma[:,:,:self.k] * n\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = TopK_custom(2,epsilon=0.001)\n",
    "a = torch.tensor([[9.75,0.42,0.01,-8.645,3.21]]).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[9.9156e-01, 0.0000e+00],\n",
       "         [3.8249e-26, 1.0894e-02],\n",
       "         [2.0042e-32, 7.9291e-06],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [8.4441e-03, 9.8910e-01]]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "topK(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}