# Human Eyes: Project 1
**姓名:何雨琪  	学号:2018202183		组号:22**

## 任务描述
在本项目Human Eye中，需要对给定的一张图片输出一个描述字幕，我们使用CNN和LSTM建立一个模型。
我们主要以论文 `Show and Tell`,以及 `Show,Attend,and Tell`为基础进行算法的实现。通过论文的学习，我们可以使用CNN来提取图片特征，同时利用RNN来生成文字序列，在第二篇论文中还利用了注意力机制，这个机制模拟了人眼观察图片说出一句话时，每个单词参考的图片区域各有侧重。

## 数据选择以及数据处理
### 数据集选择
我们可以下载到的数据集有三个`MSCOCO’14 Dataset`, `Flickr30K Datase`以及`Flickr8k Dataset`，其中MSCOCO 训练集13G，验证集6G，为了提高我们的模型训练效率，我们决定采用 Flickr8k dataset,整个数据集大小为1.12G。
对于图片描述，我们使用的是Andrej Karpathy’s training, validation, and test splits。

### 数据预处理
- 图像处理
对于一张三通道的模型，我们需要通过Encoder得到一个高维的特征表达，我们使用在ImageNet上预训练模型`resnet50`作为Encoder，对于图片输入需要做一些处理：
	1. 要求pixel的范围为[0,1],同时用ImageNet的均值和方差做标准化
	2. 将图片resize到(256,256)

- 字幕处理
	1. 对字幕设置开始标志`<start>`.，结束标志`<end>`.
	2. 将句子用`<pad>`补长或者截断到统一长度
	3. 创建单词到数字的映射 `word map`，生成`word_map.json`文件

- 字幕长度
	 每个句子的长度都是：实际长度+2

 `python data_preprocessing`
通过这个代码，会将Flickr8k Dataset 划分成训练集和验证集，代码实现内容如下：

- 读入所有图片，并resize到(256,256),同时根据图像的描述分为训练集、测试集和验证集，创建三个HDF5文件。
- 创建三个对应的JSON文件，对应训练集，验证集和测试集的字幕，字母顺序和HDF5文件中图片顺序一致。每张图片有5个字幕，每句字幕都被统一到52的长度。
- 创建另外三个JSON文件，对应训练集、验证集和测试集的字幕长度。
- 创建word_map.json。

## Encoder
Encoder主要作用是将一张输入的三通道图片编码成一个固定格式的code,这个code可以作为原始图片的一个特征表达。我们使用预训练的`ResNet-50`进行特征提取,最后的提取的特征形状为(14,14,2048)。
`test_cnn.py`
此代码是对某张图片进行特征提取，正确产生了图像特征。

## Decoder
Decoder的主要作用就是通过编码之后的图像，一步一步生成一句图像描述。因为生成的字幕是一段序列，需要使用循环神经网络，我们使用LSTMcell.
我们尚未实现Attention机制，不含Attention机制的Decoder实现在`models.py`中的Decoder。
- 使用word embedding 做词语预测：
在Decoder中，一个关键问题是表示出每个单词的词向量，我们使用word embedding来实现单词低维向量的快速输出。
在pytorch中实现word embedding是通过一个函数实现的：`nn.Embedding()`。
1. 我们需要把每个单词用一个数字标识，我们在数据预处理时已经创建了word_map
2. word embedding 的定义`nn.Embedding(vocab_size, embed_dim)`,表示将vocab_size个单词表示为embed_dim维

在 `embedding_test.py`文件中，实现了word embedding做简单单词预测，其中使用的模型是N-Gram。
```python
#定义模型
class NgramModel(nn.Module):  
    def __init__(self, vocb_size, context_size, n_dim):  
        super(NgramModel, self).__init__()  
        self.n_word = vocb_size  
        self.embedding = nn.Embedding(self.n_word, n_dim)  
        self.linear1 = nn.Linear(context_size*n_dim, 128)  
        self.linear2 = nn.Linear(128, self.n_word)  
  
    def forward(self, x):  
        emb = self.embedding(x)  
        emb = emb.view(1, -1)  
        out = self.linear1(emb)  
        out = F.relu(out)  
        out = self.linear2(out)  
        log_prob = F.log_softmax(out)  
        return log_prob
```
```python
#训练
for epoch in range(100):  
    print('epoch: {}'.format(epoch+1))  
    print('*'*10)  
    running_loss = 0  
  for data in trigram:  
        word, label = data  
        word = Variable(torch.LongTensor([word_to_idx[i] for i in word]))  
        label =Variable(torch.LongTensor([word_to_idx[label]]))  
        # forward  
		out = ngrammodel(word)  
        loss = criterion(out, label)  
        print("loss:",running_loss)  
        print(loss.data)  
        running_loss += loss.item()  
        # backward  
		optimizer.zero_grad()  
        loss.backward()  
        optimizer.step()  
    print('Loss: {:.6f}'.format(running_loss / len(word_to_idx)))
```

- 不加Attention机制的LSTM算法流程：
我们将编码之后的特征做一个全局平均池化，然后通过仿射变换之后作为隐含状态 $h_0$ 输入到 LSTM 当中，然后可以生成一个单词，同时生成下一步的隐含状态 $h_1$，接着该隐含状态和当前预测的单词作为下一次的输入，再一次输入到 LSTM 当中得到下一步的输出，通过不断的生成，直到最后模型输出一个结束标志 `<end>`，就终止模型的继续生成。
![decoder without attention](Project1_md_files/decoder_20201030183820.png?v=1&type=image&token=V1:NAa9bsZSi6gco_88dxG3aPPR8tv1aAgpiLHpJaYWCcU)
1. 需要传入LSTM一个初始的`hidden state` 和`cell state`,我们利用Encoder的输出`encoder_out`，通过两个线性层将其分别转换为`hidden state`和`cell state`,这里是通过函数`init_hidden_state()`实现。
```python
def init_hidden_state(self, encoder_out):  
    # 对所有的像素求平均  
	mean_encoder_out = encoder_out.mean(dim=1)  
    # 线性映射分别得到 hidden state 和 cell state  
    h = self.init_h(mean_encoder_out)  
    c = self.init_c(mean_encoder_out)  
    return h, c
```
2. 对输入进行一个排序,排序按照字幕长度降序排列，通过这个排序我们可以每个时间步都不用处理`<pad>`
```python
caption_lens, sort_idx = caption_lens.squeeze(1).sort(dim=0, descending=True)  
encoder_out = encoder_out[sort_idx]  
encoded_captions = encoded_captions[sort_idx]
```
3. 将embedding和cell state, hidden state输入到LSTMcell中，得到新的hidden state(output), cell state。hidden state输出得到的每个单词得分决定当前的单词选择。
```python
h, c = self.decode_step(embeddings[:batch_size_t, t,:],(h[:batch_size_t], c[:batch_size_t]))
```
- 使用Attention机制的算法流程(未实现)：
加入attention机制，序列中生成一个单词时，模型需要学会每一个单词要关注图片中的哪一个位置。解码器不再是对特征做全局平均，而是先得到一个注意力矩阵，通过注意力矩阵与特征相乘，让模型知道应该注意哪些像素点，再输入Decoder中生成单词。




## Attention
- Attention机制（待实现）
- 


